"""
æ··åˆåŠ¨åŠ›çˆ¬è™«ï¼šæ˜é‡‘(Selenium) + GitHub/HN(Requests)
"""
from __future__ import annotations

import logging
import time
from datetime import datetime
from typing import Dict, List, Optional

import requests
from bs4 import BeautifulSoup

# Selenium ç›¸å…³æ¨¡å—
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

from config import USER_AGENT
from database import get_mongo_database

logger = logging.getLogger(__name__)

# æ˜é‡‘ç½‘é¡µç‰ˆåˆ†ç±»åœ°å€
JUEJIN_URLS = {
    "åç«¯": "https://juejin.cn/backend",
    "å‰ç«¯": "https://juejin.cn/frontend",
    "AI": "https://juejin.cn/ai",
    "Android": "https://juejin.cn/android",
}

GITHUB_TRENDING_URLS = {
    "all": "https://github.com/trending",
    "python": "https://github.com/trending/python",
    "java": "https://github.com/trending/java",
    "javascript": "https://github.com/trending/javascript",
}
HACKER_NEWS_TOP = "https://hacker-news.firebaseio.com/v0/topstories.json"


def _get_collection():
    return get_mongo_database("tech_crawler")["articles"]


def _session() -> requests.Session:
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    my_proxy_port = 7897

    proxies = {
        "http": f"http://127.0.0.1:{my_proxy_port}",
        "https": f"http://127.0.0.1:{my_proxy_port}",
    }

    # æŒ‚è½½ä»£ç†
    session.proxies.update(proxies)

    return session


def _sanitize_seed(seed: str) -> str:
    cleaned = "".join(ch if ch.isalnum() else "-" for ch in seed)
    return cleaned or "tech"


def _placeholder_image(seed: str) -> str:
    return f"https://picsum.photos/seed/{_sanitize_seed(seed)}/800/400"


def _resolve_top_image(url: Optional[str], session: requests.Session, seed: str) -> str:
    if not url:
        return _placeholder_image(seed)
    try:
        resp = session.get(url, timeout=3)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        tag = soup.find("meta", property="og:image") or soup.find(
            "meta", attrs={"name": "og:image"}
        )
        if tag and tag.get("content"):
            content = tag.get("content").strip()
            if content:
                return content
    except Exception:
        pass
    return _placeholder_image(seed)


# ==========================================
# æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ Selenium çˆ¬å–æ˜é‡‘
# ==========================================
def crawl_juejin_selenium(limit_per_category: int = 15) -> List[Dict]:
    payloads: List[Dict] = []

    # é…ç½® Chrome é€‰é¡¹
    chrome_options = Options()
    # å¦‚æœæƒ³çœ‹ç€å®ƒçˆ¬ï¼ŒæŠŠä¸‹é¢è¿™è¡Œæ³¨é‡Šæ‰ï¼›å¦‚æœæƒ³åå°é™é»˜çˆ¬ï¼Œä¿ç•™è¿™è¡Œ
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # ä¼ªè£… User-Agent
    chrome_options.add_argument(f"user-agent={USER_AGENT}")

    try:
        # è‡ªåŠ¨å®‰è£…å¹¶å¯åŠ¨å¯¹åº”ç‰ˆæœ¬çš„ ChromeDriver
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(15)

        for category_name, url in JUEJIN_URLS.items():
            logger.info(f"æ­£åœ¨æ‰“å¼€æ˜é‡‘ã€{category_name}ã€‘é¡µé¢: {url}")
            try:
                driver.get(url)
                time.sleep(2) # ç­‰å¾…é¡µé¢åŠ è½½

                # æ¨¡æ‹Ÿæ»šåŠ¨ 2 æ¬¡ï¼ŒåŠ è½½æ›´å¤šæ•°æ®
                for _ in range(2):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1.5)

                # æå–æ–‡ç« åˆ—è¡¨ (æ˜é‡‘çš„ CSS ç±»åå¯èƒ½ä¼šå˜ï¼Œè¿™é‡Œä½¿ç”¨ç›¸å¯¹é€šç”¨çš„ç»“æ„)
                # entry-list ä¸‹é¢çš„ item
                articles = driver.find_elements(By.CSS_SELECTOR, ".entry-list .entry")

                count = 0
                for article in articles:
                    if count >= limit_per_category:
                        break

                    try:
                        # æ’é™¤å¹¿å‘Š
                        if "advertisement" in article.get_attribute("class"):
                            continue

                        # æå–æ ‡é¢˜å’Œé“¾æ¥
                        title_elem = article.find_element(By.CSS_SELECTOR, ".title-row a.title")
                        title = title_elem.text.strip()
                        link = title_elem.get_attribute("href")

                        # æå–æ‘˜è¦
                        try:
                            summary = article.find_element(By.CSS_SELECTOR, ".abstract a").text.strip()
                        except:
                            summary = f"{category_name} çƒ­é—¨æ–‡ç« "

                        # æå–å°é¢å›¾ (å¦‚æœæœ‰)
                        try:
                            img_elem = article.find_element(By.CSS_SELECTOR, "img.lazy")
                            cover = img_elem.get_attribute("src")
                        except:
                            cover = None

                        if not title or not link:
                            continue

                        payloads.append({
                            "title": title,
                            "url": link,
                            "summary": summary[:300],
                            "source": "juejin",
                            "tags": [category_name],
                            "top_image": cover if cover else _placeholder_image(title),
                            "publish_date": datetime.utcnow().isoformat()
                        })
                        count += 1

                    except Exception as e:
                        continue # è·³è¿‡è§£æé”™è¯¯çš„å•æ¡

                logger.info(f"æ˜é‡‘ã€{category_name}ã€‘æŠ“å–å®Œæˆï¼Œå…± {count} æ¡")

            except Exception as e:
                logger.error(f"æ˜é‡‘ã€{category_name}ã€‘é¡µé¢åŠ è½½å¤±è´¥: {e}")

        driver.quit()

    except Exception as e:
        logger.error(f"Selenium å¯åŠ¨å¤±è´¥: {e}")

    return payloads


def crawl_github_trending(session: requests.Session, per_page: int = 10) -> List[Dict]:
    payloads: List[Dict] = []
    for label, url in GITHUB_TRENDING_URLS.items():
        time.sleep(1)
        try:
            resp = session.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as exc:
            logger.error("æŠ“å– GitHub Trending %s å¤±è´¥: %s", label, exc)
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        rows = soup.select("article.Box-row")[:per_page]
        for row in rows:
            link = row.select_one("h2 a")
            desc = row.select_one("p")
            if not link:
                continue
            repo_path = link.get("href", "").strip()
            title = link.get_text(strip=True)
            repo_url = f"https://github.com{repo_path}"
            description = desc.get_text(strip=True) if desc else ""
            tags = ["GitHub Trending"]
            if label != "all":
                tags.append(label.capitalize())
            payloads.append(
                {
                    "title": title,
                    "url": repo_url,
                    "summary": description,
                    "source": "github",
                    "tags": tags,
                    "top_image": _resolve_top_image(repo_url, session, repo_path or title),
                    "publish_date": datetime.utcnow().isoformat(),
                }
            )
    return payloads


def crawl_hacker_news(session: requests.Session, limit: int = 20) -> List[Dict]:
    payloads: List[Dict] = []
    try:
        ids = session.get(HACKER_NEWS_TOP, timeout=10).json()[:limit]
    except Exception as exc:
        logger.error("è·å– Hacker News ID å¤±è´¥: %s", exc)
        return payloads
    for story_id in ids:
        detail_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        try:
            detail = session.get(detail_url, timeout=5).json()
        except Exception as exc:
            logger.warning("è·å– Hacker News %s å¤±è´¥: %s", story_id, exc)
            continue
        if not detail or "url" not in detail:
            continue
        story_url = detail["url"]
        payloads.append(
            {
                "title": detail.get("title", "Hacker News Story"),
                "url": story_url,
                "summary": "",
                "source": "hackernews",
                "tags": ["Hacker News"],
                "top_image": _resolve_top_image(story_url, session, str(story_id)),
                "publish_date": datetime.fromtimestamp(detail.get("time", 0)).isoformat()
                if detail.get("time")
                else None,
            }
        )
    return payloads


def _upsert_articles(payloads: List[Dict]):
    if not payloads:
        return
    collection = _get_collection()
    for doc in payloads:
        doc["updated_at"] = datetime.utcnow()
        collection.update_one({"url": doc["url"]}, {"$set": doc}, upsert=True)


def run_crawlers():
    session = _session()
    sources = []

    logger.info("ğŸš€ å¯åŠ¨æ··åˆçˆ¬è™«...")

    # 1. å¯åŠ¨ Selenium çˆ¬æ˜é‡‘
    logger.info("æ­£åœ¨å¯åŠ¨ Selenium çˆ¬å–æ˜é‡‘ (å¯èƒ½éœ€è¦å‡ ç§’é’Ÿå¯åŠ¨æµè§ˆå™¨)...")
    sources.extend(crawl_juejin_selenium())

    # 2. å¯åŠ¨ Requests çˆ¬ GitHub
    logger.info("æ­£åœ¨çˆ¬å– GitHub Trending...")
    sources.extend(crawl_github_trending(session))

    # 3. å¯åŠ¨ Requests çˆ¬ Hacker News
    logger.info("æ­£åœ¨çˆ¬å– Hacker News...")
    sources.extend(crawl_hacker_news(session))

    if not sources:
        logger.warning("âŒ æœ¬è½®æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼")
        return
    _upsert_articles(sources)
    logger.info("âœ… çˆ¬è™«ä»»åŠ¡å…¨éƒ¨ç»“æŸï¼Œå…±å¤„ç† %d æ¡è®°å½•ã€‚", len(sources))


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    run_crawlers()