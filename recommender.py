"""
AI æ¨¡å—ï¼šæ¯æ—¥ç§‘æŠ€æ—©æŠ¥ + å¹½é»˜è¾£è¯„æ¨èã€‚
"""
from __future__ import annotations

import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Sequence, Tuple

from openai import OpenAI
from pymongo.collection import Collection
from sqlalchemy import text

from config import LLM_API_KEY, LLM_BASE_URL, LLM_MODEL_NAME
from database import get_mongo_database, mysql_connection

logger = logging.getLogger(__name__)
_llm_client: Optional[OpenAI] = None


def _collection() -> Collection:
    return get_mongo_database("tech_crawler")["articles"]


def _is_llm_configured() -> bool:
    token = (LLM_API_KEY or "").strip()
    return bool(token) and "ä½ çš„_ModelScope_Token" not in token


def get_llm_client() -> OpenAI:
    global _llm_client
    if _llm_client is None:
        if not _is_llm_configured():
            raise RuntimeError("LLM_API_KEY æœªè®¾ç½®æˆ–ä»ä¸ºå ä½ç¬¦ã€‚")
        _llm_client = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)
    return _llm_client


def _call_llm(messages: List[Dict], max_tokens: int = 600) -> str:
    client = get_llm_client()
    response = client.chat.completions.create(
        model=LLM_MODEL_NAME,
        stream=False,
        messages=messages,
        temperature=0.4,
        max_tokens=max_tokens,
        extra_body={"enable_thinking": False},
    )
    return response.choices[0].message.content or ""


def generate_daily_flash(limit: int = 10) -> str:
    collection = _collection()
    headlines = [
        item.get("title", "ç§‘æŠ€é€Ÿé€’")
        for item in collection.find({"title": {"$ne": None}})
        .sort("updated_at", -1)
        .limit(limit)
    ]
    if not headlines:
        return "å¤§å®¶æ—©ï¼èµ„è®¯åº“ç©ºç©ºå¦‚ä¹Ÿï¼Œèµ¶ç´§è¿è¡Œçˆ¬è™«è¡¥è´§å§ â˜•ï¸"
    prompt = (
        "è¿™é‡Œæ˜¯ä»Šå¤©æœ€çƒ­çš„ç§‘æŠ€æ–°é—»æ ‡é¢˜ï¼š"
        + json.dumps(headlines, ensure_ascii=False)
        + "ã€‚è¯·æ‰®æ¼”ä¸€ä¸ªå¹½é»˜ã€å……æ»¡æ´»åŠ›çš„ç§‘æŠ€åšä¸»ï¼Œå†™ä¸€æ®µ 100 å­—å·¦å³çš„ã€æ—©æŠ¥å¹¿æ’­è¯ã€‘ã€‚"
        "é£æ ¼è¦è½»æ¾ã€å£è¯­åŒ–ï¼Œç”¨ Emojiï¼Œå¼€å¤´è¯´â€œå¤§å®¶æ—©ï¼â€ã€‚"
    )
    try:
        content = _call_llm(
            [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€åæ´»åŠ›åè¶³çš„ç§‘æŠ€åšä¸»ï¼Œç”¨ä¸­æ–‡è¾“å‡ºå¹¿æ’­è¯ã€‚",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=300,
        ).strip()
        if content:
            return content
    except Exception as exc:
        logger.error("ç”Ÿæˆæ¯æ—¥æ—©æŠ¥å¤±è´¥: %s", exc)
    return "å¤§å®¶æ—©ï¼èµ„è®¯ç«é€Ÿèµ¶æ¥ï¼Œä½† AI æœ‰ç‚¹å¡å£³ï¼Œç¨åå†è¯•è¯• ğŸ”§"


def _load_user_interests(user_id: str) -> List[str]:
    with mysql_connection() as conn:
        row = conn.execute(
            text("SELECT interests FROM users WHERE user_id = :user_id"),
            {"user_id": user_id},
        ).fetchone()
    if not row or not row[0]:
        return []
    try:
        interests = json.loads(row[0])
        return interests if isinstance(interests, list) else []
    except json.JSONDecodeError:
        return []


def _query_articles_by_tags(tags: Sequence[str], limit: int) -> List[Dict]:
    collection = _collection()
    cursor = collection.find({"tags": {"$in": list(tags)}}).sort("updated_at", -1).limit(
        limit
    )
    return list(cursor)


def _query_hot_articles(limit: int) -> List[Dict]:
    return list(_collection().find().sort("updated_at", -1).limit(limit))


def _query_mixed_candidates(limit: int) -> List[Dict]:
    collection = _collection()
    per_source = 5
    source_lists = {
        "juejin": list(
            collection.find({"source": "juejin"}).sort("updated_at", -1).limit(per_source)
        ),
        "github": list(
            collection.find({"source": "github"}).sort("updated_at", -1).limit(per_source)
        ),
        "hackernews": list(
            collection.find({"source": "hackernews"})
            .sort("updated_at", -1)
            .limit(per_source)
        ),
    }

    def sort_key(doc: Dict):
        ts = doc.get("updated_at")
        if isinstance(ts, datetime):
            return ts
        if isinstance(ts, str):
            try:
                return datetime.fromisoformat(ts)
            except ValueError:
                pass
        return datetime.min

    mixed: List[Dict] = []
    for src in ["juejin", "github", "hackernews"]:
        pool = source_lists.get(src) or []
        if pool:
            mixed.append(pool.pop(0))

    remaining: List[Dict] = []
    for pool in source_lists.values():
        remaining.extend(pool)
    remaining.sort(key=sort_key, reverse=True)

    for doc in remaining:
        if len(mixed) >= limit:
            break
        mixed.append(doc)

    if len(mixed) < limit:
        extra = _query_hot_articles(limit * 2)
        seen_urls = {doc.get("url") for doc in mixed}
        for doc in extra:
            if doc.get("url") in seen_urls:
                continue
            mixed.append(doc)
            seen_urls.add(doc.get("url"))
            if len(mixed) >= limit:
                break
    return mixed[:limit]


def _build_late_prompt(candidates: List[Dict], user_tags: List[str]) -> str:
    formatted = []
    for idx, article in enumerate(candidates, 1):
        formatted.append(
            f"ID: {idx}\n"
            f"æ ‡é¢˜: {article.get('title')}\n"
            f"ç®€ä»‹: {article.get('summary', '') or 'æš‚æ— ç®€ä»‹'}\n"
            f"æ ‡ç­¾: {', '.join(article.get('tags', [])) or 'æ— '}\n"
            f"é“¾æ¥: {article.get('url')}"
        )
    instructions = (
        "ä½ æ˜¯ä¸€ä¸ªæ¯’èˆŒã€å¹½é»˜ã€è°ƒçš®çš„æŠ€æœ¯å¤§Vã€‚"
        "è¯·ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ï¼Œç¤ºä¾‹ï¼š[{\"index\":1,\"ai_comment\":\"...\",\"tag_match\":\"Python\"}]\n"
        "index å¿…é¡»å¯¹åº”æˆ‘æä¾›çš„ IDï¼Œai_comment è¦ä¸­æ–‡ä¿çš®è¯ï¼ˆâ‰¤40å­—ï¼‰ï¼Œtag_match ç”¨äºè¯´æ˜å‘½ä¸­çš„æ ‡ç­¾æˆ–å¡«å†™â€œçƒ­é—¨æ¨èâ€ã€‚"
    )
    return (
        f"{instructions}\n\nå€™é€‰æ–‡ç« åˆ—è¡¨ï¼š\n{chr(10).join(formatted)}\n\n"
        f"ç”¨æˆ·å…³æ³¨æ ‡ç­¾ï¼š{', '.join(user_tags) if user_tags else 'æœªæŒ‡å®š'}\n"
        "è¯·ä¿è¯ JSON é¡ºåºä¸ ID é¡ºåºä¸€è‡´ã€‚"
    )


def _parse_json_response(content: str) -> List[Dict]:
    text = content.strip()
    if text.startswith("```"):
        text = text.strip("`")
        if text.startswith("json"):
            text = text[4:]
    try:
        data = json.loads(text)
        return data if isinstance(data, list) else []
    except json.JSONDecodeError as exc:
        logger.error("LLM JSON è§£æå¤±è´¥: %s", exc)
        return []


def recommend_articles(
    user_id: str, interests: Optional[List[str]] = None, limit: int = 9
) -> Tuple[List[Dict], Optional[str]]:
    interests = interests or _load_user_interests(user_id)
    articles: List[Dict] = []
    seen_urls = set()
    if interests:
        tagged = _query_articles_by_tags(interests, limit)
        articles.extend(tagged)
        seen_urls.update({item.get("url") for item in tagged if item.get("url")})
    if len(articles) < limit:
        mixed = _query_mixed_candidates(limit)
        for item in mixed:
            url = item.get("url")
            if url and url in seen_urls:
                continue
            articles.append(item)
            if url:
                seen_urls.add(url)
            if len(articles) >= limit:
                break
    articles = articles[:limit]
    if not articles:
        return [], "æ–‡ç« æ± ä¸ºç©ºï¼Œè¯·è¿è¡Œçˆ¬è™«ã€‚"

    prompt = _build_late_prompt(articles, interests)
    diagnostic: Optional[str] = None
    try:
        raw = _call_llm(
            [
                {
                    "role": "system",
                    "content": (
                        "ä½ æ˜¯ä¸€ä¸ªæ¯’èˆŒã€å¹½é»˜ã€è°ƒçš®çš„æŠ€æœ¯å¤§Vã€‚åªå›å¤ JSONï¼Œå¹¶ç¡®ä¿å­—æ®µé½å…¨ã€‚"
                    ),
                },
                {"role": "user", "content": prompt},
            ]
        )
        llm_output = _parse_json_response(raw)
    except RuntimeError as exc:
        logger.warning("LLM æœªé…ç½®: %s", exc)
        llm_output = []
        diagnostic = "LLM_API_KEY æœªé…ç½®ï¼Œå·²å›é€€è‡³çƒ­é—¨æ¨èã€‚"
    except Exception as exc:
        logger.error("LLM è°ƒç”¨å¤±è´¥: %s", exc)
        llm_output = []
        diagnostic = "AI è¾£è¯„ç”Ÿæˆå¤±è´¥ï¼Œæš‚æ—¶å±•ç¤ºçƒ­é—¨æ¨èã€‚"

    llm_map: Dict[int, Dict] = {}
    if llm_output:
        for entry in llm_output:
            idx = entry.get("index")
            if not isinstance(idx, int):
                continue
            if idx < 1 or idx > len(articles):
                continue
            if idx in llm_map:
                continue
            llm_map[idx] = entry

    results = []
    for idx, base in enumerate(articles, 1):
        entry = llm_map.get(idx)
        ai_comment = ""
        tag_match = None
        if entry:
            ai_comment = entry.get("ai_comment") or ""
            tag_match = entry.get("tag_match")
        if not ai_comment:
            ai_comment = f"æ¥è‡ª{base.get('source','èµ„è®¯')} çš„çƒ­é—¨æ¨èï¼Œåˆ«é”™è¿‡ã€‚"
        if not tag_match:
            tag_match = _resolve_tag_match(base, interests)
        results.append(
            {
                "title": base.get("title"),
                "url": base.get("url"),
                "top_image": base.get("top_image"),
                "ai_comment": ai_comment,
                "tag_match": tag_match,
            }
        )
    return results, diagnostic


def _resolve_tag_match(article: Dict, interests: Optional[List[str]]) -> str:
    tags = article.get("tags") or []
    if interests:
        for tag in tags:
            if tag in interests:
                return tag
    return "çƒ­é—¨æ¨è"


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    print(generate_daily_flash())
    print(recommend_articles("user_001"))
